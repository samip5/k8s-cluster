apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: victoria-metrics-stack
  namespace: monitoring
spec:
  interval: 30m
  chart:
    spec:
      chart: victoria-metrics-k8s-stack
      version: 0.17.1
      sourceRef:
        kind: HelmRepository
        name: victoriametrics-charts
        namespace: flux-system
  maxHistory: 2
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    fullnameOverride: victoria-metrics

    # VM Operator deployment
    victoria-metrics-operator:
      enabled: true
      operator:
        disable_prometheus_converter: false

    # Main VM Cluster in HA mode.
    vmcluster:
      enabled: true
      spec:
        # Duration to keep metrics
        retentionPeriod: "2" # this is months unless a character prefix is added (1d, 1w, etc)
        extraArgs:
          dedup.minScrapeInterval: "60s" # Only needed for HA
        replicationFactor: 3
        # Storage backend
        vmstorage:
          replicaCount: 3 # HA
          # required for HA vmagents
          extraArgs:
            dedup.minScrapeInterval: 60s # Only needed for HA
          storage:
            volumeClaimTemplate:
              spec:
                storageClassName: local-hostpath
                resources:
                  requests:
                    storage: 10Gi
        # Query backend
        vmselect:
          replicaCount: 3 # HA
          cacheMountPath: /select-cache
          extraArgs:
            dedup.minScrapeInterval: 60s # should be equal to VMAgent's scrapeInterval (default 30s)

          storage:
            volumeClaimTemplate:
              spec:
                storageClassName: local-hostpath
                resources:
                  requests:
                    storage: 2Gi
        # Data ingestion backend
        vminsert:
          replicaCount: 3 # HA
          extraArgs:
            maxLabelsPerTimeseries: "90" # More needed if you are scraping a LOT of stuff and having issues

      ingress:
        storage:
          enabled: true
          ingressClassName: cilium
          hosts:
            - &hoststorage vmstorage.skylab.fi
          tls:
            - hosts:
                - *hoststorage

        select:
          enabled: true
          ingressClassName: cilium
          hosts:
            - &hostselect vmselect.skylab.fi
          tls:
            - hosts:
                - *hostselect

        insert:
          enabled: true
          ingressClassName: cilium
          hosts:
            - &hostinsert vminsert.skylab.fi
          tls:
            - hosts:
                - *hostinsert
    # VM Alerting (however, this just watches & passes alerts to prom-alertmanager below...)
    vmalert:
      enabled: true
      spec:
        externalURL: https://vmalert.${SECRET_DOMAIN}
        notifiers:
          - selector:
              namespaceSelector:
                matchNames: [ "monitoring" ]
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: victoria-metrics-k8s-stack
      ingress:
        enabled: true
        ingressClassName: local-hostpath
        hosts:
          - &hostalert vmalert.skylab.fi
        tls:
          - hosts:
              - *hostalert

    # VM Data scraping
    vmagent:
      enabled: true
      spec:
        replicaCount: 3
        scrapeInterval: "60s"
        #additionalScrapeConfigs:
        #  name: vm-additional-scrape-configs
        #  key: prometheus-additional.yaml

      ingress:
        enabled: true
        ingressClassName: local-hostpath
        hosts:
          - &hostagent vmagent.skylab.fi
        tls:
          - hosts:
              - *hostagent
    # Single-binary vm cluster alternative (liteweight, non-HA)
    vmsingle:
      enabled: false

    # Extra slack templates
    monzoTemplate:
      enabled: false

    # Prepared sets of default rules
    # Adjust to what scraping functions you have enabled
    # i.e. if you dont have kubeapisever setup & enabled, disable
    #   the kubeApiserver rules below
    defaultRules:
      create: true
      rules:
        etcd: true
        general: true
        k8s: true
        kubeApiserver: true
        kubeApiserverAvailability: true
        kubeApiserverBurnrate: true
        kubeApiserverHistogram: true
        kubeApiserverSlos: true
        kubelet: true
        kubePrometheusGeneral: true
        kubePrometheusNodeRecording: true
        kubernetesApps: true
        kubernetesResources: true
        kubernetesStorage: true
        kubernetesSystem: true
        kubeScheduler: true
        kubeStateMetrics: true
        network: true
        node: true
        vmagent: true
        vmsingle: true
        vmhealth: true
        alertmanager: true

    # Prometheus alertmanager spinup
    # Wait, why are we using vmalert AND prometheus alertmanager???
    # VMalert isn't fully featured and VM acknowledge it's a favourite
    # for example,
    alertmanager:
      enabled: true

      ingress:
        enabled: true
        ingressClassName: cilium
        hosts:
          - &hostam alertmanager.skylab.fi
        tls:
          - hosts:
              - *hostam
      spec:
        externalURL: https://alertmanager.${EXTERNAL_DOMAIN}
        replicaCount: 3
        # Import Alertmanager config from an existing secret
        # this can be configured directly in chart, but it can
        #   be a bit unwieldy to manage
        configSecret: alertmanager-secret

        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: local-hostpath
              resources:
                requests:
                  storage: 50Mi

    # Add Kubelet scrape
    kubelet:
      enabled: true
      spec:
        metricRelabelConfigs:
          # Drop high cardinality labels
          - action: labeldrop
            regex: (uid)
          - action: labeldrop
            regex: (id|name)
          - action: drop
            source_labels: [ "__name__" ]
            regex: (rest_client_request_duration_seconds_bucket|rest_client_request_duration_seconds_sum|rest_client_request_duration_seconds_count)

    # Add Kubeapi scrape
    kubeApiServer:
      enabled: true
      spec:
        metricRelabelConfigs:
          # Drop high cardinality labels
          - sourceLabels: [ "__name__" ]
            regex: (apiserver|etcd|rest_client)_request(|_sli|_slo)_duration_seconds_bucket
            action: drop
          - sourceLabels: [ "__name__" ]
            regex: (apiserver_response_sizes_bucket|apiserver_watch_events_sizes_bucket)
            action: drop

    # Add kubeControllerManager scrape
    kubeControllerManager:
      enabled: true
      endpoints: &cp
        - 10.0.105.34
      service:
        enabled: true
        port: 10259
        targetPort: 10259
      spec:
        endpoints:
          - port: http-metrics
            scheme: http

    # Add kubeScheduler scrape
    kubeScheduler:
      enabled: false
      # endpoints: *cp
      # service:
      #   enabled: true
      #   port: 10259
      #   targetPort: 10259
      #
      # spec:
      #   endpoints:
      #     - port: http-metrics
      #       insecureSkipVerify: true

    # Add kubeEtcd scrape
    kubeEtcd:
      enabled: true
      endpoints: *cp
      service:
        enabled: true
        port: 2381
        targetPort: 2381
      spec:
        endpoints:
          - port: http-metrics
            scheme: http

    # Enable deployment of kube state metrics containers
    # I chose to deploy these separately and manage it myself
    #   instead of trusting the stack chart
    kube-state-metrics:
      enabled: false

    # Enable deployment of prom-note-exporter containers
    # I chose to deploy these separately and manage it myself
    #   instead of trusting the stack chart
    prometheus-node-exporter:
      enabled: false

    # Enable deployment of grafana
    # I chose to deploy this separately and manage it myself
    #   instead of trusting the stack chart
    grafana:
      enabled: false